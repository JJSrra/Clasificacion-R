---
title: "Clasificación R"
author: "Juanjo Sierra"
date: "4 de diciembre de 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(17)
```

# Clasificación en R

Vamos a utilizar el dataset "Breast Cancer Wisconsin Diagnostic", que cuenta con 569 ejemplos de 32 características cada uno, identificando cada uno como cáncer benigno (B) o maligno (M).

```{r}
wbcd = read.csv("Datos/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)
head(wbcd)
```

### Preprocesamiento

En primer lugar se eliminará la característica `id` que no aporta ninguna información en este caso.

```{r}
wbcd = wbcd[,-1]
head(wbcd)
```

Comprobamos cómo se distribuyen los ejemplos en base a los valores de `diagnosis`, la variable a predecir.

```{r}
table(wbcd$diagnosis)
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
```

Vamos a cambiar la columna `diagnosis` por un factor.

```{r}
wbcd$diagnosis = factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")) 
table(wbcd$diagnosis) 
```

Ahora vamos a normalizar los valores numéricos de cada columna. Observemos en primer lugar el rango de algunas de las variables numéricas.

```{r}
summary(wbcd[,c("radius_mean", "area_mean", "smoothness_mean")])
```

A continuación normalizamos los valores y nos aseguramos de que se han realizado los cambios satisfactoriamente.

```{r}
wbcd_n = as.data.frame(lapply(wbcd[,2:31], scale, center = TRUE, scale = TRUE)) 

summary(wbcd_n[,c("radius_mean", "area_mean", "smoothness_mean")]) 
boxplot(wbcd_n[,c("radius_mean", "area_mean", "smoothness_mean")]) 
```

Si mostramos las gráficas de algunas de estas variables entre sí podremos confirmar que no existe diferencia en los datos (las gráficas son iguales). Lo mismo ocurre con las correlaciones entre las variables, la normalización de los datos no las modifica.

```{r}
plot(wbcd[,2:5]) 
plot(wbcd_n[,1:4], col=wbcd[,1]) 
cor(wbcd[,2:5]) 
cor(wbcd_n[,1:4]) 
```

### Creación de particiones

A continuación vamos a separar el dataset en conjuntos de train y de test, y a guardar sus etiquetas en una variable distinta.

```{r}
shuffle_ds = sample(dim(wbcd_n)[1]) 
eightypct = (dim(wbcd_n)[1] * 80) %/% 100 

wbcd_train = wbcd_n[shuffle_ds[1:eightypct], ] 
wbcd_test = wbcd_n[shuffle_ds[(eightypct+1):dim(wbcd_n)[1]], ] 

wbcd_train_labels = wbcd[shuffle_ds[1:eightypct], 1] 
wbcd_test_labels = wbcd[shuffle_ds[(eightypct+1):dim(wbcd_n)[1]], 1]
```

Predecimos los valores de un modelo k-NN usando la función `knn` del paquete `class`.

```{r}
library(class)
wbcd_test_pred = knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
table(wbcd_test_pred,wbcd_test_labels)
```

## Ejercicio 1. Probar diferentes configuraciones de k y hacer una comparación con los resultados.

Ahora vamos a probar el mismo funcionamiento usando las funciones `train` y `predict` del paquete `caret`. Esto nos facilitará la resolución del ejercicio planteado.

En primer lugar cargamos el paquete y probamos con algunas configuraciones de k, por ejemplo entre 1 y 10.

```{r}
require(caret)
knnModel = train(wbcd_train, wbcd_train_labels, method="knn",
           metric="Accuracy", tuneGrid = data.frame(.k=1:10)) 
knnModel
```

Vemos que el mejor modelo, con el que se ha quedado la función train, es el 9-NN, sin embargo como esto puede provocar sobreajuste, vamos a escoger los 3 con mayor acierto para el estudio en test.

```{r}
mejoresK = order(knnModel$results$Accuracy, decreasing = TRUE)[1:3]
mejoresK
```

Obtenemos los modelos para cada uno de estos valores de k.

```{r}
knnModel.1 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[1]))

knnModel.2 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[2]))

knnModel.3 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[3]))
```

A continuación predecimos las etiquetas para la variable de salida con los datos de test.

```{r}
knnPred.1 = predict(knnModel.1, wbcd_test)
knnPred.2 = predict(knnModel.2, wbcd_test)
knnPred.3 = predict(knnModel.3, wbcd_test)
```

Y una vez obtenidas las etiquetas, comprobamos el acierto logrado por cada uno de los modelos en el conjunto de test.

```{r}
postResample(knnPred.1, wbcd_test_labels)
postResample(knnPred.2, wbcd_test_labels)
postResample(knnPred.3, wbcd_test_labels)
```

También podemos comparar visualmente los resultados de los modelos entre sí en una gráfica. Además se van a añadir las etiquetas originales en una última gráfica que sirva como referencia.

```{r}
par(mfrow=c(2,2))
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.1, main="k-NN Modelo 1")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.2, main="k-NN Modelo 2")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.3, main="k-NN Modelo 3")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=wbcd_test_labels, main="Originales Test")
```

