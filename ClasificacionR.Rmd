---
title: "Clasificación R"
author: "Juanjo Sierra"
date: "4 de diciembre de 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(17)
```

# Clasificación en R

Vamos a utilizar el dataset "Breast Cancer Wisconsin Diagnostic", que cuenta con 569 ejemplos de 32 características cada uno, identificando cada uno como cáncer benigno (B) o maligno (M).

```{r}
wbcd = read.csv("Datos/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)
head(wbcd)
```

### Preprocesamiento

En primer lugar se eliminará la característica `id` que no aporta ninguna información en este caso.

```{r}
wbcd = wbcd[,-1]
head(wbcd)
```

Comprobamos cómo se distribuyen los ejemplos en base a los valores de `diagnosis`, la variable a predecir.

```{r}
table(wbcd$diagnosis)
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
```

Vamos a cambiar la columna `diagnosis` por un factor.

```{r}
wbcd$diagnosis = factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant")) 
table(wbcd$diagnosis) 
```

Ahora vamos a normalizar los valores numéricos de cada columna. Observemos en primer lugar el rango de algunas de las variables numéricas.

```{r}
summary(wbcd[,c("radius_mean", "area_mean", "smoothness_mean")])
```

A continuación normalizamos los valores y nos aseguramos de que se han realizado los cambios satisfactoriamente.

```{r}
wbcd_n = as.data.frame(lapply(wbcd[,2:31], scale, center = TRUE, scale = TRUE)) 

summary(wbcd_n[,c("radius_mean", "area_mean", "smoothness_mean")]) 
boxplot(wbcd_n[,c("radius_mean", "area_mean", "smoothness_mean")]) 
```

Si mostramos las gráficas de algunas de estas variables entre sí podremos confirmar que no existe diferencia en los datos (las gráficas son iguales). Lo mismo ocurre con las correlaciones entre las variables, la normalización de los datos no las modifica.

```{r}
plot(wbcd[,2:5]) 
plot(wbcd_n[,1:4], col=wbcd[,1]) 
cor(wbcd[,2:5]) 
cor(wbcd_n[,1:4]) 
```

### Creación de particiones

A continuación vamos a separar el dataset en conjuntos de train y de test, y a guardar sus etiquetas en una variable distinta.

```{r}
shuffle_ds = sample(dim(wbcd_n)[1]) 
eightypct = (dim(wbcd_n)[1] * 80) %/% 100 

wbcd_train = wbcd_n[shuffle_ds[1:eightypct], ] 
wbcd_test = wbcd_n[shuffle_ds[(eightypct+1):dim(wbcd_n)[1]], ] 

wbcd_train_labels = wbcd[shuffle_ds[1:eightypct], 1] 
wbcd_test_labels = wbcd[shuffle_ds[(eightypct+1):dim(wbcd_n)[1]], 1]
```

Predecimos los valores de un modelo k-NN usando la función `knn` del paquete `class`.

```{r}
library(class)
wbcd_test_pred = knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
table(wbcd_test_pred,wbcd_test_labels)
```

## Ejercicio 1. Probar diferentes configuraciones de k y hacer una comparación con los resultados.

Ahora vamos a probar el mismo funcionamiento usando las funciones `train` y `predict` del paquete `caret`. Esto nos facilitará la resolución del ejercicio planteado.

En primer lugar cargamos el paquete y probamos con algunas configuraciones de k, por ejemplo entre 1 y 10.

```{r}
require(caret)
knnModel = train(wbcd_train, wbcd_train_labels, method="knn",
           metric="Accuracy", tuneGrid = data.frame(.k=1:10)) 
knnModel
```

Vemos que el mejor modelo, con el que se ha quedado la función train, es el 9-NN, sin embargo como esto puede provocar sobreajuste, vamos a escoger los 3 con mayor acierto para el estudio en test.

```{r}
mejoresK = order(knnModel$results$Accuracy, decreasing = TRUE)[1:3]
mejoresK
```

Obtenemos los modelos para cada uno de estos valores de k.

```{r}
knnModel.1 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[1]))

knnModel.2 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[2]))

knnModel.3 = train(wbcd_train, wbcd_train_labels, method="knn",
               metric="Accuracy", tuneGrid = data.frame(.k=mejoresK[3]))
```

A continuación predecimos las etiquetas para la variable de salida con los datos de test.

```{r}
knnPred.1 = predict(knnModel.1, wbcd_test)
knnPred.2 = predict(knnModel.2, wbcd_test)
knnPred.3 = predict(knnModel.3, wbcd_test)
```

Y una vez obtenidas las etiquetas, comprobamos el acierto logrado por cada uno de los modelos en el conjunto de test.

```{r}
postResample(knnPred.1, wbcd_test_labels)
postResample(knnPred.2, wbcd_test_labels)
postResample(knnPred.3, wbcd_test_labels)
```

También podemos comparar visualmente los resultados de los modelos entre sí en una gráfica. Además se van a añadir las etiquetas originales en una última gráfica que sirva como referencia.

```{r}
par(mfrow=c(2,2))
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.1, main="k-NN Modelo 1")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.2, main="k-NN Modelo 2")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=knnPred.3, main="k-NN Modelo 3")
plot(wbcd_test$texture_mean~wbcd_test$area_mean,col=wbcd_test_labels, main="Originales Test")
par(mfrow=c(1,1))
```

## Ejercicio 2. Usando el dataset Smarket, realizar una 10-fold cross-validation con regresión logística.

```{r}
library(ISLR)
head(Smarket)
```

Una vez cargado el dataset, entrenamos un modelo `glm` (regresión logística) usando de nuevo la función `train` del paquete caret. Entrenaremos con todas las columnas menos la columna `Direction`, que contiene las etiquetas, y la columna `Today`, de la que se extrae el valor de Direction y que sería perjudicial para nuestro modelo por provocar sobreaprendizaje.

```{r}
glmFit = train(Smarket[,-8:-9], y=Smarket[,9], method="glm",
         preProcess=c("center", "scale"), tuneLength=10,
         control=glm.control(maxit=500), trControl=trainControl(method = "cv")) 
glmFit
```

Vemos que el modelo obtenido no es muy bueno, apenas superando el 50% de acierto, por lo que no lo consideraremos adecuado para hacer una predicción sobre estos datos.

## Ejercicio 3. Probar LDA con todas las variables Lag del dataset Smarket. Hacer una rápida comparativa entre regresión logística y LDA. Probar QDA y comparar los tres métodos con gráficas.

Antes de utilizar LDA vamos a comprobar que las variables que serán objeto de nuestro estudio siguen una distribución normal. Para ello vamos a utilizar el test de Saphiro-Wilk.

```{r}
shapiro.test(Smarket$Lag1)
qqnorm(y=Smarket$Lag1)
qqline(y=Smarket$Lag1)

shapiro.test(Smarket$Lag2)
qqnorm(y=Smarket$Lag2)
qqline(y=Smarket$Lag2)

shapiro.test(Smarket$Lag3)
qqnorm(y=Smarket$Lag3)
qqline(y=Smarket$Lag3)

shapiro.test(Smarket$Lag4)
qqnorm(y=Smarket$Lag4)
qqline(y=Smarket$Lag4)

shapiro.test(Smarket$Lag5)
qqnorm(y=Smarket$Lag5)
qqline(y=Smarket$Lag5)
```

De acuerdo a los valores cercanos a 1 obtenidos como resultado del test de Saphiro-Wilk, y en relación a lo observado en las gráficas, podemos afirmar que las 5 variables Lag están distribuidas normalmente. El siguiente paso es asegurarnos de que la varianza es similar entre las distintas variables.

```{r}
var(Smarket$Lag1)
var(Smarket$Lag2)
var(Smarket$Lag3)
var(Smarket$Lag4)
var(Smarket$Lag5)
```

También podemos confirmarlo mediante una gráfica de tipo "box-plot".

```{r}
boxplot(Smarket[2:6])
```

Las gráficas son prácticamente equivalentes por lo que aseguramos la similitud entre las variables. Ahora podemos realizar LDA. Lo haremos con los años previos a 2005 para luego comprobar la predicción sobre 2005.

```{r}
require(MASS)
lda.fit <- lda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket, subset=Year<2005)
lda.fit
```

Visualicemos los datos del modelo para ambas clases.

```{r}

plot(lda.fit, type="both")
```

Vamos a realizar la predicción del año 2005 con el modelo anterior.

```{r}
Smarket.2005 = subset(Smarket,Year==2005) 
lda.pred = predict(lda.fit, Smarket.2005)
```

Obtenemos la matriz de confusión y el porcentaje de acierto de este modelo para el año 2005.

```{r}
table(lda.pred$class,Smarket.2005$Direction)
ldaPred = mean(lda.pred$class==Smarket.2005$Direction)
ldaPred
```

Como podemos observar no existe un acierto muy elevado, apenas de un 58%. Con la librería `klaR` podemos mostrar las variables una frente a otra y enseñar cómo divide el espacio el modelo.

```{r}
library(klaR)
partimat(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket, method="lda")
```

Vamos a comparar estos resultados con los obtenidos con regresión logística.

```{r}
glmFit$results$Accuracy
ldaPred
```

En base a estos resultados podemos afirmar que LDA se ha comportado mejor que regresión logística para este conjunto de datos.

Vamos ahora a probar los resultados con QDA. Para ello comprobamos antes si los datos dentro de la propia clase también se distribuyen de forma normal.

```{r}
Smarket.up = Smarket[Smarket$Direction == "Up",]
Smarket.down = Smarket[Smarket$Direction == "Down",]

var(Smarket.up$Lag1)
var(Smarket.up$Lag2)
var(Smarket.up$Lag3)
var(Smarket.up$Lag4)
var(Smarket.up$Lag5)

var(Smarket.down$Lag1)
var(Smarket.down$Lag2)
var(Smarket.down$Lag3)
var(Smarket.down$Lag4)
var(Smarket.down$Lag5)
```

A continuación procedemos a utilizar QDA.

```{r}
qda.fit = qda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket, subset=Year<2005)
qda.fit
```

Con el modelo generado podemos predecir los datos para el año 2005.

```{r}
qda.pred = predict(qda.fit, Smarket.2005)

table(qda.pred$class, Smarket.2005$Direction)
qdaPred = mean(qda.pred$class==Smarket.2005$Direction)
qdaPred
```

QDA no aporta mejores resultados que LDA, por tanto, a pesar de que este no es un buen modelo, sí es el mejor de los tres que se han probado en este dataset. Podemos echar un vistazo a las gráficas generadas con el paquete `klaR` para el modelo QDA.

```{r}
partimat(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket, method="qda")
```

## Ejercicio 4. Usando la información del dataset `clasif_train_alumnos.csv`, comparar LDA y QDA con Wilcoxon, realizar una comparativa múltiple usando Friedman y usar Holm para ver si hay un algoritmo ganador.

En primer lugar cargamos el nuevo dataset.

```{r}
alumnos = read.csv("Datos/clasif_train_alumnos.csv")
tabla.train = cbind(alumnos[,2:dim(alumnos)[2]])
colnames(tabla.train) = names(alumnos)[2:dim(alumnos)[2]]
rownames(tabla.train) = alumnos[,1]
```

Con los datos guardados, aplicamos el test de Wilcoxon entre LDA y QDA.

```{r}
LDAvsQDA = wilcox.test(tabla.train$out_train_lda, tabla.train$out_train_qda,
           alternative="two.sided", paired=TRUE)
Rmas = LDAvsQDA$statistic
pvalue = LDAvsQDA$p.value
LDAvsQDA = wilcox.test(tabla.train$out_train_qda, tabla.train$out_train_lda,
              alternative="two.sided", paired=TRUE)
Rmenos = LDAvsQDA$statistic
Rmas
Rmenos
pvalue
```

Dados estos resultados podemos afirmar con un 82% de confianza que existen diferencias suficientemente significativas entre LDA y QDA para este dataset.

A continuación realizaremos el test de Friedman con los datos anteriores.

```{r}
friedman.test(as.matrix(tabla.train))
```

Según el test de Friedman sin embargo no podemos asegurar lo anterior con una confianza elevada, estos algoritmos tan sólo serían distintos de forma significativa al 48% de confianza.

Por último, comparemos los algoritmos mediante el test de Holm.

```{r}
tam = dim(tabla.train)
groups = rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(tabla.train), groups, p.adjust = "holm", paired = TRUE)
```

Dado que todos los valores de la tabla resultante son cercanos al 50% no podemos afirmar que exista una gran diferencia entre los algoritmos. Por tanto, no existe un ganador claro entre ellos.
